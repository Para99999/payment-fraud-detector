# Foundation Model Configuration
foundation_model:
  name: "payment-transaction-encoder"
  architecture: "transformer"
  hidden_size: 256
  num_attention_heads: 8
  num_hidden_layers: 6
  intermediate_size: 1024
  max_position_embeddings: 512
  dropout: 0.1

# Pre-training Configuration
pretraining:
  batch_size: 64
  learning_rate: 0.0001
  num_epochs: 10
  warmup_steps: 500
  weight_decay: 0.01
  max_grad_norm: 1.0

# Fine-tuning Configuration
finetuning:
  method: "lora"  # lora or full
  batch_size: 32
  learning_rate: 0.0002
  num_epochs: 5
  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["query", "value"]

# Data Configuration
data:
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  max_sequence_length: 128

# Feature Engineering
features:
  numerical:
    - "amount"
    - "time"
    - "oldbalanceOrg"
    - "newbalanceOrig"
    - "oldbalanceDest"
    - "newbalanceDest"
  categorical:
    - "type"
    - "nameOrig"
    - "nameDest"
